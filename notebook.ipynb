{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de sentimientos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importaciones de bibliotecas y librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import numpy\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesta de datos de hdsf en dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------------------+-----------+-----------------+----------+------+-----+--------------------+--------------------+\n",
      "|   _c0|   _c1|                 _c2|        _c3|              _c4|       _c5|   _c6|  _c7|                 _c8|                 _c9|\n",
      "+------+------+--------------------+-----------+-----------------+----------+------+-----+--------------------+--------------------+\n",
      "|  null|    id|               title|publication|           author|      date|  year|month|                 url|             content|\n",
      "|103459|151908|Alton Sterling’s ...|   Guardian|   Jessica Glenza|2016-07-13|2016.0|  7.0|https://www.thegu...|The son of a Loui...|\n",
      "|103460|151909|Shakespeare’s fir...|   Guardian|             null|2016-05-25|2016.0|  5.0|https://www.thegu...|Copies of William...|\n",
      "|103461|151910|My grandmother’s ...|   Guardian|    Robert Pendry|2016-10-31|2016.0| 10.0|https://www.thegu...|Debt: $20, 000, S...|\n",
      "|103462|151911|I feared my life ...|   Guardian|   Bradford Frost|2016-11-26|2016.0| 11.0|https://www.thegu...|It was late. I wa...|\n",
      "|103463|151912|Texas man serving...|   Guardian|             null|2016-08-20|2016.0|  8.0|https://www.thegu...|A central Texas m...|\n",
      "|103464|151914|My dad’s Reagan p...|   Guardian|Steven W Thrasher|2016-11-28|2016.0| 11.0|https://www.thegu...|I have been battl...|\n",
      "|103465|151915|Flatmates of gay ...|   Guardian| Patrick Kingsley|2016-08-07|2016.0|  8.0|https://www.thegu...|Three flatmates o...|\n",
      "|103466|151916|Jaffas and darede...|   Guardian|Eleanor Ainge Roy|2016-07-22|2016.0|  7.0|https://www.thegu...|, Most people tak...|\n",
      "|103467|151917|NSA contractor ar...|   Guardian|   Ewen MacAskill|2016-10-05|2016.0| 10.0|https://www.thegu...|The FBI has arres...|\n",
      "|103468|151918|Donald Trump to d...|   Guardian|       Ben Jacobs|2016-12-24|2016.0| 12.0|https://www.thegu...|Donald Trump anno...|\n",
      "|103469|151919|Serbian Olympic r...|   Guardian|       Sean Ingle|2016-08-06|2016.0|  8.0|https://www.thegu...|Most Olympic rowe...|\n",
      "|103470|151920|The strange case ...|   Guardian|             null|2016-10-17|2016.0| 10.0|https://www.thegu...|Name: Pamela Ande...|\n",
      "|103471|151921|‘Voting for a rac...|   Guardian| Sabrina Siddiqui|2016-09-25|2016.0|  9.0|https://www.thegu...|From the moment i...|\n",
      "|103472|151923|Push for ban on d...|   Guardian|    Oliver Milman|2016-09-10|2016.0|  9.0|https://www.thegu...|A resolution to e...|\n",
      "|103473|151924|Bad moms: why mes...|   Guardian|    Lara Williams|2016-10-02|2016.0| 10.0|https://www.thegu...|Better Things ope...|\n",
      "|103474|151925|Planned Parenthoo...|   Guardian|     Molly Redden|2016-06-30|2016.0|  6.0|https://www.thegu...|Efforts are brewi...|\n",
      "|103475|151926|’Trust can be res...|   Guardian| Guardian readers|2016-10-29|2016.0| 10.0|https://www.thegu...|Halldora was born...|\n",
      "|103476|151927|Clinton endorsed ...|   Guardian|  Martin Pengelly|2016-09-24|2016.0|  9.0|https://www.thegu...|As Hillary Clinto...|\n",
      "|103477|151928|Ignore the naysay...|   Guardian|      Aryeh Neier|2016-09-16|2016.0|  9.0|https://www.thegu...|Barack Obama’s an...|\n",
      "+------+------+--------------------+-----------+-----------------+----------+------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('mmarulandc').getOrCreate()\n",
    "csv = '/user/mmarulandc/datasets/articles3.csv'\n",
    "df1 = spark.read.csv(csv)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicion de expresiones regulares para la limpieza de los contenidos de las diferentes publicaciones \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = '[^a-zA-Z ]'\n",
    "reg1 = '[\\s+]{2,}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza del DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando un dataframe que contenga los contedidos de las publicaciones hechas y limpiando el contenido de caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               clean|\n",
      "+--------------------+\n",
      "|             content|\n",
      "|The son of a Loui...|\n",
      "|Copies of William...|\n",
      "|Debt   Source Col...|\n",
      "|It was late I was...|\n",
      "|A central Texas m...|\n",
      "|I have been battl...|\n",
      "|Three flatmates o...|\n",
      "| Most people take...|\n",
      "|The FBI has arres...|\n",
      "|Donald Trump anno...|\n",
      "|Most Olympic rowe...|\n",
      "|Name Pamela Ander...|\n",
      "|From the moment i...|\n",
      "|A resolution to e...|\n",
      "|Better Things ope...|\n",
      "|Efforts are brewi...|\n",
      "|Halldora was born...|\n",
      "|As Hillary Clinto...|\n",
      "|Barack Obamas ann...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = df1.withColumn(\"clean\", regexp_replace('_c9', reg ,\"\"))\n",
    "df_1.select('clean').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes lineas se eliminan las espacios en blanco(\\s) de mas que son tokenisados como tokens independientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              clean1|\n",
      "+--------------------+\n",
      "|             content|\n",
      "|The son of a Loui...|\n",
      "|Copies of William...|\n",
      "|Debt Source Colle...|\n",
      "|It was late I was...|\n",
      "|A central Texas m...|\n",
      "|I have been battl...|\n",
      "|Three flatmates o...|\n",
      "| Most people take...|\n",
      "|The FBI has arres...|\n",
      "|Donald Trump anno...|\n",
      "|Most Olympic rowe...|\n",
      "|Name Pamela Ander...|\n",
      "|From the moment i...|\n",
      "|A resolution to e...|\n",
      "|Better Things ope...|\n",
      "|Efforts are brewi...|\n",
      "|Halldora was born...|\n",
      "|As Hillary Clinto...|\n",
      "|Barack Obamas ann...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = df_1.withColumn(\"clean1\", regexp_replace('clean', reg1 ,\" \"))\n",
    "df_2.select('clean1').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización de los contenidos de las publicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creacion de un dataframe con el contenido de la publicacion tokenizado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df=tokenization.transform(df_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='clean1',outputCol='tokens')\n",
    "# tokenization=Tokenizer(inputCol='_c9',outputCol='tokens')\n",
    "#tokenized_df=tokenization.transform(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              tokens|\n",
      "+--------------------+\n",
      "|           [content]|\n",
      "|[the, son, of, a,...|\n",
      "|[copies, of, will...|\n",
      "|[debt, source, co...|\n",
      "|[it, was, late, i...|\n",
      "|[a, central, texa...|\n",
      "|[i, have, been, b...|\n",
      "|[three, flatmates...|\n",
      "|[, most, people, ...|\n",
      "|[the, fbi, has, a...|\n",
      "|[donald, trump, a...|\n",
      "|[most, olympic, r...|\n",
      "|[name, pamela, an...|\n",
      "|[from, the, momen...|\n",
      "|[a, resolution, t...|\n",
      "|[better, things, ...|\n",
      "|[efforts, are, br...|\n",
      "|[halldora, was, b...|\n",
      "|[as, hillary, cli...|\n",
      "|[barack, obamas, ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.select('tokens').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoción de stopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminacion de stopWord de los contenidos de las publicaciones token tales como \"I, and .or\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      refined_tokens|\n",
      "+--------------------+\n",
      "|           [content]|\n",
      "|[son, louisiana, ...|\n",
      "|[copies, william,...|\n",
      "|[debt, source, co...|\n",
      "|[late, drunk, nea...|\n",
      "|[central, texas, ...|\n",
      "|[battling, depres...|\n",
      "|[three, flatmates...|\n",
      "|[, people, take, ...|\n",
      "|[fbi, arrested, n...|\n",
      "|[donald, trump, a...|\n",
      "|[olympic, rowers,...|\n",
      "|[name, pamela, an...|\n",
      "|[moment, may, don...|\n",
      "|[resolution, end,...|\n",
      "|[better, things, ...|\n",
      "|[efforts, brewing...|\n",
      "|[halldora, born, ...|\n",
      "|[hillary, clinton...|\n",
      "|[barack, obamas, ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.select(['refined_tokens']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 _c9|               clean|              clean1|      refined_tokens|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|The son of a Loui...|The son of a Loui...|The son of a Loui...|[son, louisiana, ...|\n",
      "|Debt: $20, 000, S...|Debt   Source Col...|Debt Source Colle...|[debt, source, co...|\n",
      "|It was late. I wa...|It was late I was...|It was late I was...|[late, drunk, nea...|\n",
      "|I have been battl...|I have been battl...|I have been battl...|[battling, depres...|\n",
      "|Three flatmates o...|Three flatmates o...|Three flatmates o...|[three, flatmates...|\n",
      "|, Most people tak...| Most people take...| Most people take...|[, people, take, ...|\n",
      "|The FBI has arres...|The FBI has arres...|The FBI has arres...|[fbi, arrested, n...|\n",
      "|Donald Trump anno...|Donald Trump anno...|Donald Trump anno...|[donald, trump, a...|\n",
      "|Most Olympic rowe...|Most Olympic rowe...|Most Olympic rowe...|[olympic, rowers,...|\n",
      "|From the moment i...|From the moment i...|From the moment i...|[moment, may, don...|\n",
      "|A resolution to e...|A resolution to e...|A resolution to e...|[resolution, end,...|\n",
      "|Better Things ope...|Better Things ope...|Better Things ope...|[better, things, ...|\n",
      "|Efforts are brewi...|Efforts are brewi...|Efforts are brewi...|[efforts, brewing...|\n",
      "|Halldora was born...|Halldora was born...|Halldora was born...|[halldora, born, ...|\n",
      "|As Hillary Clinto...|As Hillary Clinto...|As Hillary Clinto...|[hillary, clinton...|\n",
      "|Barack Obama’s an...|Barack Obamas ann...|Barack Obamas ann...|[barack, obamas, ...|\n",
      "|The Golden State ...|The Golden State ...|The Golden State ...|[golden, state, w...|\n",
      "|What happens in a...|What happens in a...|What happens in a...|[happens, hour, m...|\n",
      "|Once you’ve maste...|Once youve master...|Once youve master...|[youve, mastered,...|\n",
      "|What have you acc...|What have you acc...|What have you acc...|[accomplished, we...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_data = refined_df.dropna()\n",
    "refined_data.select('_c9','clean','clean1','refined_tokens').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer (AQUI ESTA FALLANDO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toma la cuenta de el numero de palabras que aparecen en un documento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer(inputCol='refined_tokens',\n",
    "outputCol='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'isNotNull'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-3f37b17367f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  cv_df=count_vec.fit(refined_data).transform(tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrefined_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefined_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'refined_tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'isNotNull'"
     ]
    }
   ],
   "source": [
    "cv_df=count_vec.fit(refined_data).transform(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
